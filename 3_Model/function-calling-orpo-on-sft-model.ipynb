{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORPO Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fine-tuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SFT Model and Tokenizer\n",
    "In this case a supervised fine-tuned locally saved model was used for further ORPO fine-tuning. It might not be necessary to have this previous SFT step. In case of the base model functional tokens have to be added and the weights have to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151650, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151650, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"german-function-calling-model-0.5b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"german-function-calling-model-0.5b\")\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add BOS Token\n",
    "This step was for some reasons only necessary for Qwen2-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n",
    "tokenizer.bos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the DPO/ORPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Training im Fitnessstudio\", \"2026-11-2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Training inFitness Studio\", \"2026-11-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorlesung Mittelhochdeutsch\", \"2023-02...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorbesprechung Bachelorarbeit\", \"2026-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorbesprechung Bachelorarbeit\", \"2025-...</td>\n",
       "      <td>&lt;oc_1&gt;(\"VorbeprechbungBachelorzeit\", \"2024-09-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Weihnachtsessen bei meinen Eltern\", \"2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Weihnachtssuch\", \"2025-10-13\", \"12:00\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"wöchentliches Teammeeting\", \"2023-12-2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"wöschtragenen Termin mit ID b867c9bdae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2022-10-13\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2025-10-13\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2023-12-04\", True)&lt;oc_end&gt;\\nFunktionsb...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-12-05\", True)&lt;oc_end&gt;\\nFunktionsb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2022-12-07\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-12-07\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-08-19\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-08-19\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-06-05\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-06-05\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "1     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "3     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "4     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "...                                                 ...   \n",
       "2995  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2996  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2997  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2998  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2999  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "\n",
       "                                                 chosen  \\\n",
       "0     <oc_1>(\"Training im Fitnessstudio\", \"2026-11-2...   \n",
       "1     <oc_1>(\"Vorlesung Mittelhochdeutsch\", \"2023-02...   \n",
       "2     <oc_1>(\"Vorbesprechung Bachelorarbeit\", \"2025-...   \n",
       "3     <oc_1>(\"Weihnachtsessen bei meinen Eltern\", \"2...   \n",
       "4     <oc_1>(\"wöchentliches Teammeeting\", \"2023-12-2...   \n",
       "...                                                 ...   \n",
       "2995  <oc_3>(\"2022-10-13\", False)<oc_end>\\nFunktions...   \n",
       "2996  <oc_3>(\"2023-12-04\", True)<oc_end>\\nFunktionsb...   \n",
       "2997  <oc_3>(\"2022-12-07\", False)<oc_end>\\nFunktions...   \n",
       "2998  <oc_3>(\"2024-08-19\", False)<oc_end>\\nFunktions...   \n",
       "2999  <oc_3>(\"2024-06-05\", False)<oc_end>\\nFunktions...   \n",
       "\n",
       "                                               rejected  \n",
       "0     <oc_1>(\"Training inFitness Studio\", \"2026-11-2...  \n",
       "1     <oc_1>(\"Vorbesprechung Bachelorarbeit\", \"2026-...  \n",
       "2     <oc_1>(\"VorbeprechbungBachelorzeit\", \"2024-09-...  \n",
       "3     <oc_1>(\"Weihnachtssuch\", \"2025-10-13\", \"12:00\"...  \n",
       "4     <oc_1>(\"wöschtragenen Termin mit ID b867c9bdae...  \n",
       "...                                                 ...  \n",
       "2995  <oc_3>(\"2025-10-13\", False)<oc_end>\\nFunktions...  \n",
       "2996  <oc_3>(\"2026-12-05\", True)<oc_end>\\nFunktionsb...  \n",
       "2997  <oc_3>(\"2026-12-07\", False)<oc_end>\\nFunktions...  \n",
       "2998  <oc_3>(\"2024-08-19\", False)<oc_end>\\nFunktions...  \n",
       "2999  <oc_3>(\"2026-06-05\", False)<oc_end>\\nFunktions...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dpo_dataset = pd.read_csv(\"german-dpo-dataset.csv\", sep=\";\", names=[\"prompt\", \"chosen\", \"rejected\"], skiprows=1)\n",
    "german_dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Dataset for ORPO Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<oc_1>(\"Training inFitness Studio\", \"2026-11-28\", \"20:00\", 21)<oc_end>\\nFunktionsbeschreibung: def create_calender_entry(title, date, time, duration):\\n\"\"\"\\nErstellt einen Kalendereintrag mit den angegebenen Details und ruft die Kalender-API auf.\\n\\nParameter:\\ntitle (str): Der Titel des Kalendereintrags.\\ndate (str): Das Datum des Kalendereintrags im Format \\'YYYY-MM-DD\\'.\\ntime (str): Die Uhrzeit des Kalendereintrags im Format \\'HH:MM\\'.\\nduration (int): Die Dauer des Kalendereintrags in Minuten.\\n\\nRückgabe:\\nbool: True, wenn der Aufruf der Kalender-API erfolgreich war, andernfalls False.\\n\"\"\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# german_dpo_dataset.chosen = german_dpo_dataset.chosen.apply(lambda x: x.split(\"\\n\")[0])\n",
    "\n",
    "def reformat_rejected(row):\n",
    "    chosen_parts = row[\"chosen\"].split(\"\\n\")\n",
    "    rejected_parts = row[\"rejected\"].split(\"\\n\")\n",
    "    chosen_parts[0] = rejected_parts[0]\n",
    "    return \"\\n\".join(chosen_parts)\n",
    "\n",
    "german_dpo_dataset.rejected = german_dpo_dataset.apply(reformat_rejected, axis=1)\n",
    "german_dpo_dataset.rejected.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# german_dpo_dataset.chosen = german_dpo_dataset.chosen.apply(lambda x: x.split(\"\\n\")[0])\n",
    "# german_dpo_dataset.rejected = german_dpo_dataset.rejected.apply(lambda x: x.split(\"\\n\")[0])\n",
    "# german_dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Subset of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "\n",
    "sample1 = german_dpo_dataset.iloc[:1000].sample(n=num_samples, random_state=1)\n",
    "sample2 = german_dpo_dataset.iloc[1000:2000].sample(n=num_samples, random_state=1)\n",
    "sample3 = german_dpo_dataset.iloc[2000:].sample(n=num_samples, random_state=1)\n",
    "\n",
    "# Combine the samples\n",
    "result = pd.concat([sample1, sample2, sample3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Training im Fitnessstudio\", \"2026-11-2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Training inFitness Studio\", \"2026-11-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorlesung Mittelhochdeutsch\", \"2023-02...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorbesprechung Bachelorarbeit\", \"2026-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Vorbesprechung Bachelorarbeit\", \"2025-...</td>\n",
       "      <td>&lt;oc_1&gt;(\"VorbeprechbungBachelorzeit\", \"2024-09-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Weihnachtsessen bei meinen Eltern\", \"2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"Weihnachtssuch\", \"2025-10-13\", \"12:00\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_1&gt;(\"wöchentliches Teammeeting\", \"2023-12-2...</td>\n",
       "      <td>&lt;oc_1&gt;(\"wöschtragenen Termin mit ID b867c9bdae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2022-10-13\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2025-10-13\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2023-12-04\", True)&lt;oc_end&gt;\\nFunktionsb...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-12-05\", True)&lt;oc_end&gt;\\nFunktionsb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2022-12-07\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-12-07\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-08-19\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-08-19\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Unten steht ein Befehl des Benutzer, bitte wäh...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2024-06-05\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "      <td>&lt;oc_3&gt;(\"2026-06-05\", False)&lt;oc_end&gt;\\nFunktions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "1     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "3     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "4     Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "...                                                 ...   \n",
       "2995  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2996  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2997  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2998  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "2999  Unten steht ein Befehl des Benutzer, bitte wäh...   \n",
       "\n",
       "                                                 chosen  \\\n",
       "0     <oc_1>(\"Training im Fitnessstudio\", \"2026-11-2...   \n",
       "1     <oc_1>(\"Vorlesung Mittelhochdeutsch\", \"2023-02...   \n",
       "2     <oc_1>(\"Vorbesprechung Bachelorarbeit\", \"2025-...   \n",
       "3     <oc_1>(\"Weihnachtsessen bei meinen Eltern\", \"2...   \n",
       "4     <oc_1>(\"wöchentliches Teammeeting\", \"2023-12-2...   \n",
       "...                                                 ...   \n",
       "2995  <oc_3>(\"2022-10-13\", False)<oc_end>\\nFunktions...   \n",
       "2996  <oc_3>(\"2023-12-04\", True)<oc_end>\\nFunktionsb...   \n",
       "2997  <oc_3>(\"2022-12-07\", False)<oc_end>\\nFunktions...   \n",
       "2998  <oc_3>(\"2024-08-19\", False)<oc_end>\\nFunktions...   \n",
       "2999  <oc_3>(\"2024-06-05\", False)<oc_end>\\nFunktions...   \n",
       "\n",
       "                                               rejected  \n",
       "0     <oc_1>(\"Training inFitness Studio\", \"2026-11-2...  \n",
       "1     <oc_1>(\"Vorbesprechung Bachelorarbeit\", \"2026-...  \n",
       "2     <oc_1>(\"VorbeprechbungBachelorzeit\", \"2024-09-...  \n",
       "3     <oc_1>(\"Weihnachtssuch\", \"2025-10-13\", \"12:00\"...  \n",
       "4     <oc_1>(\"wöschtragenen Termin mit ID b867c9bdae...  \n",
       "...                                                 ...  \n",
       "2995  <oc_3>(\"2025-10-13\", False)<oc_end>\\nFunktions...  \n",
       "2996  <oc_3>(\"2026-12-05\", True)<oc_end>\\nFunktionsb...  \n",
       "2997  <oc_3>(\"2026-12-07\", False)<oc_end>\\nFunktions...  \n",
       "2998  <oc_3>(\"2024-08-19\", False)<oc_end>\\nFunktions...  \n",
       "2999  <oc_3>(\"2026-06-05\", False)<oc_end>\\nFunktions...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Unten steht ein Befehl des Benutzer, bitte wähle die korrekte Funktion aus und generiere Parameter, um die Funktion aufzurufen.\\nBefehl: Erstelle einen neuen Kalendareintrag für Weihnachtsessen bei meinen Eltern am 07.09.2025 um 12 Uhr für 155 Minuten\\nAntwort: ',\n",
       " 'chosen': '<oc_1>(\"Weihnachtsessen bei meinen Eltern\", \"2025-09-07\", \"12:00\", 155)<oc_end>\\nFunktionsbeschreibung: def create_calender_entry(title, date, time, duration):\\n\"\"\"\\nErstellt einen Kalendereintrag mit den angegebenen Details und ruft die Kalender-API auf.\\n\\nParameter:\\ntitle (str): Der Titel des Kalendereintrags.\\ndate (str): Das Datum des Kalendereintrags im Format \\'YYYY-MM-DD\\'.\\ntime (str): Die Uhrzeit des Kalendereintrags im Format \\'HH:MM\\'.\\nduration (int): Die Dauer des Kalendereintrags in Minuten.\\n\\nRückgabe:\\nbool: True, wenn der Aufruf der Kalender-API erfolgreich war, andernfalls False.\\n\"\"\"',\n",
       " 'rejected': '<oc_1>(\"Weihnachtssuch\", \"2024-09-07\", \"12:00\", 163)<oc_end>\\nFunktionsbeschreibung: def create_calender_entry(title, date, time, duration):\\n\"\"\"\\nErstellt einen Kalendereintrag mit den angegebenen Details und ruft die Kalender-API auf.\\n\\nParameter:\\ntitle (str): Der Titel des Kalendereintrags.\\ndate (str): Das Datum des Kalendereintrags im Format \\'YYYY-MM-DD\\'.\\ntime (str): Die Uhrzeit des Kalendereintrags im Format \\'HH:MM\\'.\\nduration (int): Die Dauer des Kalendereintrags in Minuten.\\n\\nRückgabe:\\nbool: True, wenn der Aufruf der Kalender-API erfolgreich war, andernfalls False.\\n\"\"\"'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from trl import ORPOTrainer, ORPOConfig\n",
    "\n",
    "dataset = Dataset.from_dict(result)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fine-tuning/.venv/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:209: UserWarning: `max_length` is not set in the ORPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fine-tuning/.venv/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:218: UserWarning: `max_prompt_length` is not set in the ORPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fine-tuning/.venv/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 707.88 examples/s]\n",
      "[codecarbon INFO @ 13:44:25] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:44:25] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:44:25] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:44:25] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:44:25] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 13:44:26] We saw that you have a Intel Xeon Processor (Icelake) but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 13:44:26] CPU Model on constant consumption mode: Intel Xeon Processor (Icelake)\n",
      "[codecarbon INFO @ 13:44:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:44:26]   Platform system: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 13:44:26]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:44:26]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 13:44:26]   Available RAM : 31.337 GB\n",
      "[codecarbon INFO @ 13:44:26]   CPU count: 16\n",
      "[codecarbon INFO @ 13:44:26]   CPU model: Intel Xeon Processor (Icelake)\n",
      "[codecarbon INFO @ 13:44:26]   GPU count: 1\n",
      "[codecarbon INFO @ 13:44:26]   GPU model: 1 x NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "dpo_config = ORPOConfig(\n",
    "    output_dir=\"german-function-calling-orpo-0.5b\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    optim = \"adamw_8bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps = 10,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    warmup_steps=10,\n",
    "    beta=0.1, # the lambda/alpha hyperparameter in the paper/code\n",
    "    seed = 3407,\n",
    ")\n",
    "\n",
    "dpo_trainer = ORPOTrainer(\n",
    "    model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:44:42] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.751500129699707 W\n",
      "[codecarbon INFO @ 13:44:42] Energy consumed for all GPUs : 0.000889 kWh. Total GPU Power : 213.1950908169809 W\n",
      "[codecarbon INFO @ 13:44:42] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:44:42] 0.001115 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:44:57] Energy consumed for RAM : 0.000098 kWh. RAM Power : 11.751500129699707 W\n",
      "[codecarbon INFO @ 13:44:57] Energy consumed for all GPUs : 0.001791 kWh. Total GPU Power : 216.6229861550458 W\n",
      "[codecarbon INFO @ 13:44:57] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:44:57] 0.002243 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:45:12] Energy consumed for RAM : 0.000147 kWh. RAM Power : 11.751500129699707 W\n",
      "[codecarbon INFO @ 13:45:12] Energy consumed for all GPUs : 0.002701 kWh. Total GPU Power : 218.64036134529871 W\n",
      "[codecarbon INFO @ 13:45:12] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:45:12] 0.003379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:45:27] Energy consumed for RAM : 0.000196 kWh. RAM Power : 11.751500129699707 W\n",
      "[codecarbon INFO @ 13:45:27] Energy consumed for all GPUs : 0.003595 kWh. Total GPU Power : 214.8154113689722 W\n",
      "[codecarbon INFO @ 13:45:27] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:45:27] 0.004500 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:45:34] Energy consumed for RAM : 0.000219 kWh. RAM Power : 11.751500129699707 W\n",
      "[codecarbon INFO @ 13:45:34] Energy consumed for all GPUs : 0.003813 kWh. Total GPU Power : 111.77089859244474 W\n",
      "[codecarbon INFO @ 13:45:34] Energy consumed for all CPUs : 0.000791 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:45:34] 0.004823 kWh of electricity used since the beginning.\n",
      "/home/ubuntu/fine-tuning/.venv/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.1163454802831014, metrics={'train_runtime': 67.0414, 'train_samples_per_second': 8.95, 'train_steps_per_second': 4.475, 'total_flos': 0.0, 'train_loss': 0.1163454802831014, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model and the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model(\"german-function-calling-orpo-0.5b-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('german-function-calling-orpo-0.5b-model/tokenizer_config.json',\n",
       " 'german-function-calling-orpo-0.5b-model/special_tokens_map.json',\n",
       " 'german-function-calling-orpo-0.5b-model/vocab.json',\n",
       " 'german-function-calling-orpo-0.5b-model/merges.txt',\n",
       " 'german-function-calling-orpo-0.5b-model/added_tokens.json',\n",
       " 'german-function-calling-orpo-0.5b-model/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"german-function-calling-orpo-0.5b-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
      "Befehl: Liste alle meine noch offenen Kalendareinträge für den 24.06.2024 auf\n",
      "Antwort: <oc_3>(\"2024-05\", True)<oc_end>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# prompt = \"\"\"Below is the query from the users, please choose the correct function and generate the\n",
    "# parameters to call the function.\n",
    "# Query: Create an appointment 'Business Lunch' on 2024-06-21 at 10:00 for 120 minutes.\n",
    "# Response: \"\"\"\n",
    "\n",
    "prompt = \"\"\"Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
    "Befehl: {query}\n",
    "Antwort: \"\"\".format(query=\"Liste alle meine noch offenen Kalendareinträge für den 24.06.2024 auf\")\n",
    "\n",
    "early_stopping_token = \"<oc_end>\"\n",
    "eos_token_id = tokenizer.encode(early_stopping_token, add_special_tokens=False)[0]\n",
    "eos_token_id\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.0,\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=2.0,\n",
    "    max_new_tokens=64,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=eos_token_id  # Set the early stopping token ID\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
      "Befehl: Erstelle einen neuen Kalendareintrag für eine Projektpräsentation am 24.06.2024 um 18 Uhr für 90 Minuten\n",
      "Antwort: <oc_1>(\"Projektpräsentereinträge\", \"2024-05-34\", '18:00', 70)<oc_end>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# prompt = \"\"\"Below is the query from the users, please choose the correct function and generate the\n",
    "# parameters to call the function.\n",
    "# Query: Create an appointment 'Business Lunch' on 2024-06-21 at 10:00 for 120 minutes.\n",
    "# Response: \"\"\"\n",
    "\n",
    "prompt = \"\"\"Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
    "Befehl: {query}\n",
    "Antwort: \"\"\".format(query=\"Erstelle einen neuen Kalendareintrag für eine Projektpräsentation am 24.06.2024 um 18 Uhr für 90 Minuten\")\n",
    "\n",
    "early_stopping_token = \"<oc_end>\"\n",
    "eos_token_id = tokenizer.encode(early_stopping_token, add_special_tokens=False)[0]\n",
    "eos_token_id\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.0,\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=2.0,\n",
    "    max_new_tokens=64,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=eos_token_id  # Set the early stopping token ID\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
      "Befehl: Lösche den Kalendareintrag mit ID 39asdaf972\n",
      "Antwort: <oc_2>(\"cerndafb972\")<oc_end>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# prompt = \"\"\"Below is the query from the users, please choose the correct function and generate the\n",
    "# parameters to call the function.\n",
    "# Query: Create an appointment 'Business Lunch' on 2024-06-21 at 10:00 for 120 minutes.\n",
    "# Response: \"\"\"\n",
    "\n",
    "prompt = \"\"\"Unten befindet sich der Befehl des Benutzers, wähle bitte die passende Funktion aus und generiere die Parameter für die Funktion.\n",
    "Befehl: {query}\n",
    "Antwort: \"\"\".format(query=\"Lösche den Kalendareintrag mit ID 39asdaf972\")\n",
    "\n",
    "early_stopping_token = \"<oc_end>\"\n",
    "eos_token_id = tokenizer.encode(early_stopping_token, add_special_tokens=False)[0]\n",
    "eos_token_id\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.0,\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=2.0,\n",
    "    max_new_tokens=64,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=eos_token_id  # Set the early stopping token ID\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
